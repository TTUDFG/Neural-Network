{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcd5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11838ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Get the image and change every image to an 256-dimention vector\n",
    "dataSet = np.zeros([10, 256])\n",
    "for i in range(0, 10):\n",
    "    inputImageDir = './input/' + str(i) + '.png'\n",
    "    inputImage = Image.open(inputImageDir)\n",
    "    inputImage = inputImage.convert(\"1\")\n",
    "    inputImage.save(inputImageDir)\n",
    "    data = inputImage.getdata()\n",
    "    array = np.array(data)/255\n",
    "    dataSet[i] = array\n",
    "dataSet = np.array(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1478743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# define a neural network\n",
    "class Perceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        self.activate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.linear(x)\n",
    "        res = self.activate(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "634a9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, dataset, label_list):\n",
    "        self.dataset = dataset\n",
    "        self.label_list = label_list\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        label = self.label_list[idx]\n",
    "        return {\n",
    "            'data': torch.from_numpy(data).float(),\n",
    "            'label': torch.from_numpy(label).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc25a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of training\n",
    "input_size = 256\n",
    "num_classes = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 600\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = DigitDataset(dataset = dataSet, label_list = dataSet)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cpu')\n",
    "model = Perceptron(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "if not os.path.exists('./models'):\n",
    "    os.mkdir('./models')\n",
    "torch.save(model, './models/net_untrained.pkl')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452b7bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/600] Loss: 0.2661 MAE: 0.4995 Mean Error: 0.4124 STD: 0.3100\n",
      "[10/600] Loss: 0.0441 MAE: 0.1588 Mean Error: 0.0965 STD: 0.1866\n",
      "[20/600] Loss: 0.0308 MAE: 0.0915 Mean Error: 0.0354 STD: 0.1720\n",
      "[30/600] Loss: 0.0282 MAE: 0.0782 Mean Error: 0.0211 STD: 0.1666\n",
      "[40/600] Loss: 0.0265 MAE: 0.0734 Mean Error: 0.0170 STD: 0.1618\n",
      "[50/600] Loss: 0.0249 MAE: 0.0707 Mean Error: 0.0144 STD: 0.1573\n",
      "[60/600] Loss: 0.0236 MAE: 0.0689 Mean Error: 0.0136 STD: 0.1529\n",
      "[70/600] Loss: 0.0223 MAE: 0.0673 Mean Error: 0.0131 STD: 0.1486\n",
      "[80/600] Loss: 0.0210 MAE: 0.0656 Mean Error: 0.0128 STD: 0.1445\n",
      "[90/600] Loss: 0.0199 MAE: 0.0640 Mean Error: 0.0124 STD: 0.1405\n",
      "[100/600] Loss: 0.0188 MAE: 0.0624 Mean Error: 0.0120 STD: 0.1367\n",
      "[110/600] Loss: 0.0178 MAE: 0.0609 Mean Error: 0.0118 STD: 0.1329\n",
      "[120/600] Loss: 0.0169 MAE: 0.0593 Mean Error: 0.0116 STD: 0.1293\n",
      "[130/600] Loss: 0.0160 MAE: 0.0577 Mean Error: 0.0113 STD: 0.1259\n",
      "[140/600] Loss: 0.0151 MAE: 0.0563 Mean Error: 0.0110 STD: 0.1225\n",
      "[150/600] Loss: 0.0143 MAE: 0.0548 Mean Error: 0.0107 STD: 0.1193\n",
      "[160/600] Loss: 0.0136 MAE: 0.0534 Mean Error: 0.0105 STD: 0.1162\n",
      "[170/600] Loss: 0.0129 MAE: 0.0521 Mean Error: 0.0103 STD: 0.1132\n",
      "[180/600] Loss: 0.0123 MAE: 0.0508 Mean Error: 0.0100 STD: 0.1103\n",
      "[190/600] Loss: 0.0116 MAE: 0.0495 Mean Error: 0.0098 STD: 0.1075\n",
      "[200/600] Loss: 0.0111 MAE: 0.0483 Mean Error: 0.0096 STD: 0.1048\n",
      "[210/600] Loss: 0.0105 MAE: 0.0471 Mean Error: 0.0093 STD: 0.1022\n",
      "[220/600] Loss: 0.0100 MAE: 0.0459 Mean Error: 0.0091 STD: 0.0997\n",
      "[230/600] Loss: 0.0095 MAE: 0.0448 Mean Error: 0.0089 STD: 0.0973\n",
      "[240/600] Loss: 0.0091 MAE: 0.0438 Mean Error: 0.0088 STD: 0.0949\n",
      "[250/600] Loss: 0.0087 MAE: 0.0428 Mean Error: 0.0086 STD: 0.0927\n",
      "[260/600] Loss: 0.0083 MAE: 0.0418 Mean Error: 0.0084 STD: 0.0906\n",
      "[270/600] Loss: 0.0079 MAE: 0.0409 Mean Error: 0.0082 STD: 0.0886\n",
      "[280/600] Loss: 0.0076 MAE: 0.0399 Mean Error: 0.0081 STD: 0.0866\n",
      "[290/600] Loss: 0.0072 MAE: 0.0391 Mean Error: 0.0079 STD: 0.0847\n",
      "[300/600] Loss: 0.0069 MAE: 0.0382 Mean Error: 0.0078 STD: 0.0829\n",
      "[310/600] Loss: 0.0066 MAE: 0.0374 Mean Error: 0.0076 STD: 0.0811\n",
      "[320/600] Loss: 0.0064 MAE: 0.0367 Mean Error: 0.0075 STD: 0.0794\n",
      "[330/600] Loss: 0.0061 MAE: 0.0359 Mean Error: 0.0073 STD: 0.0778\n",
      "[340/600] Loss: 0.0059 MAE: 0.0352 Mean Error: 0.0072 STD: 0.0763\n",
      "[350/600] Loss: 0.0056 MAE: 0.0345 Mean Error: 0.0071 STD: 0.0748\n",
      "[360/600] Loss: 0.0054 MAE: 0.0338 Mean Error: 0.0070 STD: 0.0733\n",
      "[370/600] Loss: 0.0052 MAE: 0.0332 Mean Error: 0.0068 STD: 0.0719\n",
      "[380/600] Loss: 0.0050 MAE: 0.0326 Mean Error: 0.0067 STD: 0.0706\n",
      "[390/600] Loss: 0.0048 MAE: 0.0320 Mean Error: 0.0066 STD: 0.0693\n",
      "[400/600] Loss: 0.0047 MAE: 0.0314 Mean Error: 0.0065 STD: 0.0680\n",
      "[410/600] Loss: 0.0045 MAE: 0.0308 Mean Error: 0.0064 STD: 0.0668\n",
      "[420/600] Loss: 0.0043 MAE: 0.0303 Mean Error: 0.0063 STD: 0.0657\n",
      "[430/600] Loss: 0.0042 MAE: 0.0298 Mean Error: 0.0062 STD: 0.0645\n",
      "[440/600] Loss: 0.0041 MAE: 0.0293 Mean Error: 0.0061 STD: 0.0634\n",
      "[450/600] Loss: 0.0039 MAE: 0.0288 Mean Error: 0.0060 STD: 0.0624\n",
      "[460/600] Loss: 0.0038 MAE: 0.0283 Mean Error: 0.0060 STD: 0.0613\n",
      "[470/600] Loss: 0.0037 MAE: 0.0279 Mean Error: 0.0059 STD: 0.0603\n",
      "[480/600] Loss: 0.0036 MAE: 0.0274 Mean Error: 0.0058 STD: 0.0594\n",
      "[490/600] Loss: 0.0034 MAE: 0.0270 Mean Error: 0.0057 STD: 0.0584\n",
      "[500/600] Loss: 0.0033 MAE: 0.0266 Mean Error: 0.0056 STD: 0.0575\n",
      "[510/600] Loss: 0.0032 MAE: 0.0262 Mean Error: 0.0056 STD: 0.0567\n",
      "[520/600] Loss: 0.0031 MAE: 0.0258 Mean Error: 0.0055 STD: 0.0558\n",
      "[530/600] Loss: 0.0031 MAE: 0.0254 Mean Error: 0.0054 STD: 0.0550\n",
      "[540/600] Loss: 0.0030 MAE: 0.0250 Mean Error: 0.0053 STD: 0.0542\n",
      "[550/600] Loss: 0.0029 MAE: 0.0247 Mean Error: 0.0053 STD: 0.0534\n",
      "[560/600] Loss: 0.0028 MAE: 0.0243 Mean Error: 0.0052 STD: 0.0526\n",
      "[570/600] Loss: 0.0027 MAE: 0.0240 Mean Error: 0.0052 STD: 0.0519\n",
      "[580/600] Loss: 0.0026 MAE: 0.0237 Mean Error: 0.0051 STD: 0.0512\n",
      "[590/600] Loss: 0.0026 MAE: 0.0233 Mean Error: 0.0050 STD: 0.0505\n",
      "[599/600] Loss: 0.0025 MAE: 0.0231 Mean Error: 0.0050 STD: 0.0498\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "def train(dataloader, model, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        ERROR_Train = [] \n",
    "        model.train() \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            model.zero_grad()\n",
    "            real_cpu, label_cpu = data['data'], data['label']\n",
    "#             if torch.cuda.is_available():\n",
    "#                 real_cpu = real_cpu.cuda() \n",
    "#                 label_cpu = label_cpu.cuda()\n",
    "            real = real_cpu\n",
    "            label = label_cpu\n",
    "            inputv = Variable(real)\n",
    "            labelv = Variable(label)\n",
    "            output = model(inputv)\n",
    "            err = criterion(output, labelv) \n",
    "            err.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            losses.append(err.data.item())\n",
    "            error = label - output.data\n",
    "#             print(error.shape)\n",
    "            ERROR_Train.extend(error)\n",
    "#         print(ERROR_Train)\n",
    "        MAE = torch.mean(torch.abs(torch.stack(ERROR_Train)))\n",
    "        ME = torch.mean(torch.stack(ERROR_Train))\n",
    "        STD = torch.std(torch.stack(ERROR_Train)) \n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print('[%d/%d] Loss: %.4f MAE: %.4f Mean Error: %.4f STD: %.4f' % (epoch, num_epochs, np.average(losses), MAE, ME, STD))\n",
    "    return output, model\n",
    "\n",
    "# Start training        \n",
    "output, model = train(train_loader, model, num_epochs)\n",
    "# print(output.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f4d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256)\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "# Step 4a\n",
    "# Export the image after training\n",
    "# Before executing this block, create a folder called \"output\"\n",
    "if not os.path.exists('./output'):\n",
    "    os.mkdir('./output')\n",
    "output_np = output.detach().numpy()\n",
    "print(output_np.shape)\n",
    "torch.save(model, './models/net_trained.pkl')\n",
    "output_dataset = np.zeros([10, 256])\n",
    "for i in range(10):\n",
    "    output_img = output_np[i].reshape(16, 16)*255\n",
    "    img = Image.fromarray(np.uint8(output_img))\n",
    "    img = img.convert(\"1\")\n",
    "    output_path = './output/' + str(i) + '.png'\n",
    "    img.save(output_path)\n",
    "    data = img.getdata()\n",
    "    array = np.array(data)/255\n",
    "    output_dataset[i] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b891b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b\n",
    "# Calculate Fh\n",
    "def calculateFh(input_dataset, output_dataset):\n",
    "    x, y = input_dataset.shape\n",
    "    Fh_denominator = 0    # Fh分母\n",
    "    Fh_numerator = 0      # Fh分子\n",
    "    Fh_array = np.zeros([x])\n",
    "    for j in range(x):\n",
    "        for i in range(y):\n",
    "            if input_dataset[j][i] == 0:\n",
    "                Fh_denominator = Fh_denominator + 1\n",
    "                if output_dataset[j][i] == 0:\n",
    "                    Fh_numerator = Fh_numerator + 1\n",
    "        Fh = Fh_numerator / Fh_denominator\n",
    "        Fh_array[j] = Fh\n",
    "    return Fh_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3acfb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Ffa\n",
    "def calculateFfa(input_dataset, output_dataset):\n",
    "    x, y = input_dataset.shape\n",
    "    Ffa_denominator = 0    # Ffa分母\n",
    "    Ffa_numerator = 0      # Ffa分子\n",
    "    Ffa_array = np.zeros([x])\n",
    "    for j in range(x):\n",
    "        for i in range(y):\n",
    "            if input_dataset[j][i] == 1:\n",
    "                Ffa_denominator = Ffa_denominator + 1\n",
    "            if output_dataset[j][i] == 0 and input_dataset[j][i] == 1:\n",
    "                Ffa_numerator = Ffa_numerator + 1\n",
    "        Ffa = Ffa_numerator / Ffa_denominator\n",
    "        Ffa_array[j] = Ffa\n",
    "    return Ffa_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d276af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0.004329   0.00210526 0.00141243 0.0021254  0.00170213 0.00142146\n",
      " 0.001221   0.0010644  0.00094877 0.00085507]\n"
     ]
    }
   ],
   "source": [
    "Fh_array = calculateFh(dataSet, output_dataset)\n",
    "Ffa_array = calculateFfa(dataSet, output_dataset)\n",
    "print(Fh_array)\n",
    "print(Ffa_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "097f693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4c: Graph Fh as a function of Ffa for each exemplar in the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01286dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with noise standard deviation 0.001\n",
      "[0/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[10/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[20/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[30/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[40/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[50/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[60/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[70/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[80/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[90/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[100/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[110/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[120/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[130/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[140/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[150/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[160/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[170/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[180/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[190/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[200/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[210/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[220/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[230/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[240/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[250/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[260/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[270/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[280/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[290/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[300/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[310/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[320/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[330/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[340/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[350/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[360/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[370/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[380/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[390/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[400/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[410/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[420/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[430/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[440/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[450/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[460/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[470/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[480/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[490/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[500/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[510/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[520/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[530/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[540/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[550/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[560/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[570/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[580/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[590/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "[599/600] Loss: 0.2688 MAE: 0.5036 Mean Error: 0.4191 STD: 0.3052\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.002\n",
      "[0/600] Loss: 0.2726 MAE: 0.5058 Mean Error: 0.4197 STD: 0.3107\n",
      "[10/600] Loss: 0.2726 MAE: 0.5058 Mean Error: 0.4197 STD: 0.3107\n",
      "[20/600] Loss: 0.2726 MAE: 0.5058 Mean Error: 0.4197 STD: 0.3107\n",
      "[30/600] Loss: 0.2726 MAE: 0.5058 Mean Error: 0.4197 STD: 0.3107\n",
      "[40/600] Loss: 0.2726 MAE: 0.5058 Mean Error: 0.4197 STD: 0.3107\n",
      "[50/600] Loss: 0.2726 MAE: 0.5058 Mean Error: 0.4197 STD: 0.3107\n",
      "[60/600] Loss: 0.2726 MAE: 0.5058 Mean Error: 0.4197 STD: 0.3107\n"
     ]
    }
   ],
   "source": [
    "# Step 5\n",
    "def gaussian_noise(img, mean, sigma):\n",
    "    # Generate gauss noise\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    # Add the noise to image\n",
    "    gaussian_out = img + noise\n",
    "    # Make the value between 0 and 1\n",
    "    gaussian_out = np.clip(gaussian_out, 0, 1)\n",
    "    return gaussian_out\n",
    "\n",
    "gaussian_dataset = np.zeros([9, 10, 256])\n",
    "std = [0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "if not os.path.exists('./input_noise/'):\n",
    "    os.mkdir('./input_noise/')\n",
    "for j in range(9):\n",
    "    if not os.path.exists('./input_noise/' + str(std[j])):\n",
    "        os.mkdir('./input_noise/' + str(std[j]))\n",
    "    for i in range(10):\n",
    "        inputImage = dataSet[i]\n",
    "        gaussian_data = gaussian_noise(inputImage, 0, std[j])\n",
    "        img = gaussian_data.reshape(16, 16)*255\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        img.convert(\"1\")\n",
    "        inputImageDir = './input_noise/' + str(std[j]) + '/' + str(i) + '.png'\n",
    "        img.save(inputImageDir)\n",
    "        gaussian_dataset[j][i] = gaussian_data\n",
    "gaussian_dataset = np.array(gaussian_dataset)\n",
    "\n",
    "\n",
    "Fh_noise_array = np.zeros([9, 10])\n",
    "Ffa_noise_array = np.zeros([9, 10])\n",
    "\n",
    "# Train 9 datasets with noise\n",
    "if not os.path.exists('./output_noise/'):\n",
    "    os.mkdir('./output_noise/')\n",
    "for j in range(9):\n",
    "    model = Perceptron(input_size=input_size, num_classes=num_classes).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_noise_dataset = DigitDataset(dataset = gaussian_dataset[j], label_list = dataSet)\n",
    "    train_noise_loader = DataLoader(dataset=train_noise_dataset, batch_size=batch_size, shuffle=False)\n",
    "    print('Training dataset with noise standard deviation ' + str(std[j]))\n",
    "#     model_noise = torch.load('./models/net_untrained.pkl') #  Load the model that trained before\n",
    "    model_noise = Perceptron(input_size=input_size, num_classes=num_classes).to(device)\n",
    "    output_noise, model_noise = train(train_noise_loader, model_noise, num_epochs)   # Train\n",
    "    torch.save(model_noise, './models/net_trained_' + str(std[j]) + '.pkl')\n",
    "    model_noise = torch.load('./models/net_trained_' + str(std[j]) + '.pkl')\n",
    "#     output_noise = model_noise(torch.from_numpy(gaussian_dataset[j]).float()) # Use the model trained before to test\n",
    "    print('------------------------------------')\n",
    "    output_noise = model_noise(torch.from_numpy(dataSet).float())\n",
    "    output_noise_np = output_noise.detach().numpy()     # Get the output\n",
    "#     print(output_noise_np)\n",
    "    output_noise_dataset = np.zeros([10, 256])\n",
    "#     Make the output only has 0 or 1\n",
    "    \n",
    "    if not os.path.exists('./output_noise/' + str(std[j])):\n",
    "        os.mkdir('./output_noise/' + str(std[j]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(10):\n",
    "        output_noise_img = output_noise_np[i].reshape(16, 16)*255\n",
    "        img = Image.fromarray(np.uint8(output_noise_img))\n",
    "        img = img.convert(\"1\")\n",
    "        output_path = './output_noise/' + str(std[j]) + '/' + str(i) + '.png'\n",
    "        img.save(output_path)\n",
    "        data = img.getdata()\n",
    "        array = np.array(data)/255\n",
    "        output_noise_dataset[i] = array\n",
    "#     Calculate Fh and Ffa\n",
    "    Fh = calculateFh(dataSet, output_noise_dataset)\n",
    "    Ffa = calculateFfa(dataSet, output_noise_dataset)\n",
    "    Fh_noise_array[j] = Fh\n",
    "    Ffa_noise_array[j] = Ffa\n",
    "print('------------Fh_noise_array------------')\n",
    "print(Fh_noise_array)\n",
    "print('------------Ffa_noise_array------------')\n",
    "print(Ffa_noise_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4010e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 6: Display Data from your Tests in Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b32ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
