{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2abcc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4f5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Get the image and change every image to an 256-dimention vector\n",
    "dataSet = np.zeros([10, 256])\n",
    "for i in range(0, 10):\n",
    "    inputImageDir = './input/' + str(i) + '.png'\n",
    "    inputImage = Image.open(inputImageDir)\n",
    "    inputImage = inputImage.convert(\"1\")\n",
    "    inputImage.save(inputImageDir)\n",
    "    data = inputImage.getdata()\n",
    "    array = np.array(data)/255\n",
    "    dataSet[i] = array\n",
    "dataSet = np.array(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f55f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# define a neural network\n",
    "class Perceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        self.activate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.linear(x)\n",
    "        res = self.activate(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce7e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, dataset, label_list):\n",
    "        self.dataset = dataset\n",
    "        self.label_list = label_list\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        label = self.label_list[idx]\n",
    "        return {\n",
    "            'data': torch.from_numpy(data).float(),\n",
    "            'label': torch.from_numpy(label).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eebe268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of training\n",
    "input_size = 256\n",
    "num_classes = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 600\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = DigitDataset(dataset = dataSet, label_list = dataSet)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Perceptron(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c653a506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/600] Loss: 0.2723 MAE: 0.5046 Mean Error: 0.4148 STD: 0.3166\n",
      "[10/600] Loss: 0.0444 MAE: 0.1607 Mean Error: 0.1019 STD: 0.1846\n",
      "[20/600] Loss: 0.0309 MAE: 0.0909 Mean Error: 0.0354 STD: 0.1722\n",
      "[30/600] Loss: 0.0283 MAE: 0.0776 Mean Error: 0.0195 STD: 0.1670\n",
      "[40/600] Loss: 0.0264 MAE: 0.0732 Mean Error: 0.0152 STD: 0.1619\n",
      "[50/600] Loss: 0.0249 MAE: 0.0712 Mean Error: 0.0151 STD: 0.1572\n",
      "[60/600] Loss: 0.0235 MAE: 0.0694 Mean Error: 0.0138 STD: 0.1529\n",
      "[70/600] Loss: 0.0223 MAE: 0.0676 Mean Error: 0.0130 STD: 0.1486\n",
      "[80/600] Loss: 0.0210 MAE: 0.0660 Mean Error: 0.0125 STD: 0.1445\n",
      "[90/600] Loss: 0.0199 MAE: 0.0644 Mean Error: 0.0122 STD: 0.1405\n",
      "[100/600] Loss: 0.0188 MAE: 0.0628 Mean Error: 0.0120 STD: 0.1367\n",
      "[110/600] Loss: 0.0178 MAE: 0.0611 Mean Error: 0.0117 STD: 0.1329\n",
      "[120/600] Loss: 0.0168 MAE: 0.0595 Mean Error: 0.0114 STD: 0.1293\n",
      "[130/600] Loss: 0.0159 MAE: 0.0580 Mean Error: 0.0112 STD: 0.1258\n",
      "[140/600] Loss: 0.0151 MAE: 0.0565 Mean Error: 0.0110 STD: 0.1224\n",
      "[150/600] Loss: 0.0143 MAE: 0.0550 Mean Error: 0.0107 STD: 0.1191\n",
      "[160/600] Loss: 0.0135 MAE: 0.0536 Mean Error: 0.0105 STD: 0.1159\n",
      "[170/600] Loss: 0.0128 MAE: 0.0522 Mean Error: 0.0103 STD: 0.1129\n",
      "[180/600] Loss: 0.0122 MAE: 0.0508 Mean Error: 0.0101 STD: 0.1100\n",
      "[190/600] Loss: 0.0116 MAE: 0.0495 Mean Error: 0.0099 STD: 0.1071\n",
      "[200/600] Loss: 0.0110 MAE: 0.0483 Mean Error: 0.0097 STD: 0.1045\n",
      "[210/600] Loss: 0.0105 MAE: 0.0471 Mean Error: 0.0095 STD: 0.1019\n",
      "[220/600] Loss: 0.0100 MAE: 0.0459 Mean Error: 0.0093 STD: 0.0994\n",
      "[230/600] Loss: 0.0095 MAE: 0.0448 Mean Error: 0.0091 STD: 0.0970\n",
      "[240/600] Loss: 0.0090 MAE: 0.0438 Mean Error: 0.0089 STD: 0.0947\n",
      "[250/600] Loss: 0.0086 MAE: 0.0428 Mean Error: 0.0087 STD: 0.0925\n",
      "[260/600] Loss: 0.0082 MAE: 0.0418 Mean Error: 0.0085 STD: 0.0904\n",
      "[270/600] Loss: 0.0079 MAE: 0.0409 Mean Error: 0.0084 STD: 0.0884\n",
      "[280/600] Loss: 0.0075 MAE: 0.0399 Mean Error: 0.0082 STD: 0.0864\n",
      "[290/600] Loss: 0.0072 MAE: 0.0391 Mean Error: 0.0081 STD: 0.0845\n",
      "[300/600] Loss: 0.0069 MAE: 0.0382 Mean Error: 0.0079 STD: 0.0828\n",
      "[310/600] Loss: 0.0066 MAE: 0.0374 Mean Error: 0.0078 STD: 0.0810\n",
      "[320/600] Loss: 0.0064 MAE: 0.0367 Mean Error: 0.0076 STD: 0.0794\n",
      "[330/600] Loss: 0.0061 MAE: 0.0359 Mean Error: 0.0075 STD: 0.0778\n",
      "[340/600] Loss: 0.0059 MAE: 0.0352 Mean Error: 0.0074 STD: 0.0762\n",
      "[350/600] Loss: 0.0056 MAE: 0.0345 Mean Error: 0.0072 STD: 0.0747\n",
      "[360/600] Loss: 0.0054 MAE: 0.0339 Mean Error: 0.0071 STD: 0.0733\n",
      "[370/600] Loss: 0.0052 MAE: 0.0332 Mean Error: 0.0070 STD: 0.0719\n",
      "[380/600] Loss: 0.0050 MAE: 0.0326 Mean Error: 0.0069 STD: 0.0706\n",
      "[390/600] Loss: 0.0048 MAE: 0.0320 Mean Error: 0.0068 STD: 0.0693\n",
      "[400/600] Loss: 0.0047 MAE: 0.0314 Mean Error: 0.0067 STD: 0.0680\n",
      "[410/600] Loss: 0.0045 MAE: 0.0309 Mean Error: 0.0066 STD: 0.0668\n",
      "[420/600] Loss: 0.0044 MAE: 0.0304 Mean Error: 0.0065 STD: 0.0657\n",
      "[430/600] Loss: 0.0042 MAE: 0.0298 Mean Error: 0.0064 STD: 0.0645\n",
      "[440/600] Loss: 0.0041 MAE: 0.0293 Mean Error: 0.0063 STD: 0.0634\n",
      "[450/600] Loss: 0.0039 MAE: 0.0288 Mean Error: 0.0062 STD: 0.0624\n",
      "[460/600] Loss: 0.0038 MAE: 0.0284 Mean Error: 0.0061 STD: 0.0614\n",
      "[470/600] Loss: 0.0037 MAE: 0.0279 Mean Error: 0.0060 STD: 0.0604\n",
      "[480/600] Loss: 0.0036 MAE: 0.0275 Mean Error: 0.0059 STD: 0.0594\n",
      "[490/600] Loss: 0.0035 MAE: 0.0271 Mean Error: 0.0058 STD: 0.0585\n",
      "[500/600] Loss: 0.0033 MAE: 0.0266 Mean Error: 0.0058 STD: 0.0576\n",
      "[510/600] Loss: 0.0032 MAE: 0.0262 Mean Error: 0.0057 STD: 0.0567\n",
      "[520/600] Loss: 0.0031 MAE: 0.0258 Mean Error: 0.0056 STD: 0.0558\n",
      "[530/600] Loss: 0.0031 MAE: 0.0255 Mean Error: 0.0055 STD: 0.0550\n",
      "[540/600] Loss: 0.0030 MAE: 0.0251 Mean Error: 0.0055 STD: 0.0542\n",
      "[550/600] Loss: 0.0029 MAE: 0.0247 Mean Error: 0.0054 STD: 0.0534\n",
      "[560/600] Loss: 0.0028 MAE: 0.0244 Mean Error: 0.0053 STD: 0.0527\n",
      "[570/600] Loss: 0.0027 MAE: 0.0240 Mean Error: 0.0053 STD: 0.0519\n",
      "[580/600] Loss: 0.0026 MAE: 0.0237 Mean Error: 0.0052 STD: 0.0512\n",
      "[590/600] Loss: 0.0026 MAE: 0.0234 Mean Error: 0.0051 STD: 0.0505\n",
      "[599/600] Loss: 0.0025 MAE: 0.0231 Mean Error: 0.0051 STD: 0.0499\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "def train(dataloader, model, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        ERROR_Train = [] \n",
    "        model.train() \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            model.zero_grad()\n",
    "            real_cpu, label_cpu = data['data'], data['label']\n",
    "            if torch.cuda.is_available():\n",
    "                real_cpu = real_cpu.cuda() \n",
    "                label_cpu = label_cpu.cuda()\n",
    "            real = real_cpu\n",
    "            label = label_cpu\n",
    "            inputv = Variable(real)\n",
    "            labelv = Variable(label)\n",
    "            output = model(inputv)\n",
    "            err = criterion(output, labelv) \n",
    "            err.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            losses.append(err.data.item())\n",
    "            error = label - output.data\n",
    "#             print(error.shape)\n",
    "            ERROR_Train.extend(error)\n",
    "#         print(ERROR_Train)\n",
    "        MAE = torch.mean(torch.abs(torch.stack(ERROR_Train)))\n",
    "        ME = torch.mean(torch.stack(ERROR_Train))\n",
    "        STD = torch.std(torch.stack(ERROR_Train)) \n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print('[%d/%d] Loss: %.4f MAE: %.4f Mean Error: %.4f STD: %.4f' % (epoch, num_epochs, np.average(losses), MAE, ME, STD))\n",
    "    return output\n",
    "\n",
    "# Start training        \n",
    "output = train(train_loader, model, num_epochs)\n",
    "# print(output.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d367612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256)\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "# Step 4a\n",
    "# Export the image after training\n",
    "# Before executing this block, create a folder called \"output\"\n",
    "if not os.path.exists('./output'):\n",
    "    os.mkdir('./output')\n",
    "output_np = output.detach().numpy()\n",
    "print(output_np.shape)\n",
    "torch.save(model, 'net.pkl')\n",
    "output_dataset = np.zeros([10, 256])\n",
    "for i in range(10):\n",
    "    output_img = output_np[i].reshape(16, 16)*255\n",
    "    img = Image.fromarray(np.uint8(output_img))\n",
    "    img = img.convert(\"1\")\n",
    "    output_path = './output/' + str(i) + '.png'\n",
    "    img.save(output_path)\n",
    "    data = img.getdata()\n",
    "    array = np.array(data)/255\n",
    "    output_dataset[i] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48edd1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b\n",
    "# Calculate Fh\n",
    "def calculateFh(input_dataset, output_dataset):\n",
    "    x, y = input_dataset.shape\n",
    "    Fh_denominator = 0    # Fh分母\n",
    "    Fh_numerator = 0      # Fh分子\n",
    "    Fh_array = np.zeros([x])\n",
    "    for j in range(x):\n",
    "        for i in range(y):\n",
    "            if input_dataset[j][i] == 0:\n",
    "                Fh_denominator = Fh_denominator + 1\n",
    "                if output_dataset[j][i] == 0:\n",
    "                    Fh_numerator = Fh_numerator + 1\n",
    "        Fh = Fh_numerator / Fh_denominator\n",
    "        Fh_array[j] = Fh\n",
    "    return Fh_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5c63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Ffa\n",
    "def calculateFfa(input_dataset, output_dataset):\n",
    "    x, y = input_dataset.shape\n",
    "    Ffa_denominator = 0    # Ffa分母\n",
    "    Ffa_numerator = 0      # Ffa分子\n",
    "    Ffa_array = np.zeros([x])\n",
    "    for j in range(x):\n",
    "        for i in range(y):\n",
    "            if input_dataset[j][i] == 1:\n",
    "                Ffa_denominator = Ffa_denominator + 1\n",
    "            if output_dataset[j][i] == 0 and input_dataset[j][i] == 1:\n",
    "                Ffa_numerator = Ffa_numerator + 1\n",
    "        Ffa = Ffa_numerator / Ffa_denominator\n",
    "        Ffa_array[j] = Ffa\n",
    "    return Ffa_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6287172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0.         0.         0.         0.0010627  0.00085106 0.00071073\n",
      " 0.0006105  0.0005322  0.00047438 0.00042753]\n"
     ]
    }
   ],
   "source": [
    "Fh_array = calculateFh(dataSet, output_dataset)\n",
    "Ffa_array = calculateFfa(dataSet, output_dataset)\n",
    "print(Fh_array)\n",
    "print(Ffa_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f946bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4c: Graph Fh as a function of Ffa for each exemplar in the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "154e1b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Fh_noise_array------------\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "------------Ffa_noise_array------------\n",
      "[[0.         0.         0.         0.00210084 0.0016835  0.00141443\n",
      "  0.00121507 0.00107296 0.00095969 0.00085763]\n",
      " [0.         0.         0.         0.00211416 0.00168067 0.00139082\n",
      "  0.00120192 0.00105708 0.00094251 0.00084818]\n",
      " [0.         0.         0.         0.00211416 0.00169205 0.00139665\n",
      "  0.00120482 0.00105374 0.00093721 0.00084388]\n",
      " [0.         0.         0.         0.00211864 0.00170068 0.00141443\n",
      "  0.00120337 0.00106045 0.00093721 0.0008547 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00221239 0.00176056 0.00147275\n",
      "  0.00126103 0.00108696 0.00096246 0.00086881]\n",
      " [0.00763359 0.00398406 0.00263852 0.00201207 0.00161031 0.0013624\n",
      "  0.0011919  0.00104275 0.00094518 0.00085251]\n",
      " [0.00813008 0.0042735  0.00289855 0.00436681 0.00350263 0.00286944\n",
      "  0.00242718 0.00210748 0.00188324 0.00169062]]\n"
     ]
    }
   ],
   "source": [
    "# Step 5\n",
    "def gaussian_noise(img, mean, sigma):\n",
    "    # Generate gauss noise\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    # Add the noise to image\n",
    "    gaussian_out = img + noise\n",
    "    # Make the value between 0 and 1\n",
    "    gaussian_out = np.clip(gaussian_out, 0, 1)\n",
    "    return gaussian_out\n",
    "\n",
    "gaussian_dataset = np.zeros([9, 10, 256])\n",
    "std = [0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "if not os.path.exists('./input_noise/'):\n",
    "    os.mkdir('./input_noise/')\n",
    "for j in range(9):\n",
    "    if not os.path.exists('./input_noise/' + str(std[j])):\n",
    "        os.mkdir('./input_noise/' + str(std[j]))\n",
    "    for i in range(10):\n",
    "        inputImage = dataSet[i]\n",
    "        gaussian_data = gaussian_noise(inputImage, 0, std[j])\n",
    "        img = gaussian_data.reshape(16, 16)*255\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        img.convert(\"1\")\n",
    "        inputImageDir = './input_noise/' + str(std[j]) + '/' + str(i) + '.png'\n",
    "        img.save(inputImageDir)\n",
    "        gaussian_dataset[j][i] = gaussian_data\n",
    "gaussian_dataset = np.array(gaussian_dataset)\n",
    "\n",
    "\n",
    "Fh_noise_array = np.zeros([9, 10])\n",
    "Ffa_noise_array = np.zeros([9, 10])\n",
    "\n",
    "# Train 9 datasets with noise\n",
    "if not os.path.exists('./output_noise/'):\n",
    "    os.mkdir('./output_noise/')\n",
    "for j in range(9):\n",
    "    train_noise_dataset = DigitDataset(dataset = gaussian_dataset[j], label_list = gaussian_dataset[j])\n",
    "    train_noise_loader = DataLoader(dataset=train_noise_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     print('Training dataset with noise standard deviation ' + str(std[j]))\n",
    "    model_noise = torch.load('net.pkl') #  Load the model that trained before\n",
    "#     output_noise = train(train_noise_loader, model_noise, num_epochs)   # Train\n",
    "    output_noise = model_noise(torch.from_numpy(gaussian_dataset[j]).float()) # Use the model trained before to test\n",
    "#     print('------------------------------------')\n",
    "    output_noise_np = output_noise.detach().numpy()     # Get the output\n",
    "    output_noise_dataset = np.zeros([10, 256])\n",
    "#     Make the output only has 0 or 1\n",
    "    if not os.path.exists('./output_noise/' + str(std[j])):\n",
    "        os.mkdir('./output_noise/' + str(std[j]))\n",
    "    for i in range(10):\n",
    "        output_noise_img = output_noise_np[i].reshape(16, 16)*255\n",
    "        img = Image.fromarray(np.uint8(output_noise_img))\n",
    "        img = img.convert(\"1\")\n",
    "        output_path = './output_noise/' + str(std[j]) + '/' + str(i) + '.png'\n",
    "        img.save(output_path)\n",
    "        data = img.getdata()\n",
    "        array = np.array(data)/255\n",
    "        output_noise_dataset[i] = array\n",
    "#     Calculate Fh and Ffa\n",
    "    Fh = calculateFh(gaussian_dataset[j], output_noise_dataset)\n",
    "    Ffa = calculateFfa(gaussian_dataset[j], output_noise_dataset)\n",
    "    Fh_noise_array[j] = Fh\n",
    "    Ffa_noise_array[j] = Ffa\n",
    "print('------------Fh_noise_array------------')\n",
    "print(Fh_noise_array)\n",
    "print('------------Ffa_noise_array------------')\n",
    "print(Ffa_noise_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dd78210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 6: Display Data from your Tests in Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584ac1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
