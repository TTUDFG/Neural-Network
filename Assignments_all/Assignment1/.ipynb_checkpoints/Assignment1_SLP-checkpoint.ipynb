{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcd5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11838ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Get the image and change every image to an 256-dimention vector\n",
    "dataSet = np.zeros([10, 256])\n",
    "for i in range(0, 10):\n",
    "    inputImageDir = './input/' + str(i) + '.png'\n",
    "    inputImage = Image.open(inputImageDir)\n",
    "    inputImage = inputImage.convert(\"1\")\n",
    "    inputImage.save(inputImageDir)\n",
    "    data = inputImage.getdata()\n",
    "    array = np.array(data)/255\n",
    "    dataSet[i] = array\n",
    "dataSet = np.array(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1478743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# define a neural network\n",
    "class Perceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        self.activate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.linear(x)\n",
    "        res = self.activate(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "634a9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, dataset, label_list):\n",
    "        self.dataset = dataset\n",
    "        self.label_list = label_list\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        label = self.label_list[idx]\n",
    "        return {\n",
    "            'data': torch.from_numpy(data).float(),\n",
    "            'label': torch.from_numpy(label).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc25a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of training\n",
    "input_size = 256\n",
    "num_classes = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 600\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = DigitDataset(dataset = dataSet, label_list = dataSet)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cpu')\n",
    "model = Perceptron(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "if not os.path.exists('./models'):\n",
    "    os.mkdir('./models')\n",
    "torch.save(model, './models/net_untrained.pkl')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452b7bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/600] Loss: 0.2445 MAE: 0.4787 Mean Error: 0.3893 STD: 0.3049\n",
      "[10/600] Loss: 0.0422 MAE: 0.1510 Mean Error: 0.0901 STD: 0.1848\n",
      "[20/600] Loss: 0.0306 MAE: 0.0902 Mean Error: 0.0325 STD: 0.1719\n",
      "[30/600] Loss: 0.0281 MAE: 0.0784 Mean Error: 0.0200 STD: 0.1663\n",
      "[40/600] Loss: 0.0264 MAE: 0.0733 Mean Error: 0.0161 STD: 0.1617\n",
      "[50/600] Loss: 0.0249 MAE: 0.0702 Mean Error: 0.0145 STD: 0.1571\n",
      "[60/600] Loss: 0.0235 MAE: 0.0683 Mean Error: 0.0136 STD: 0.1526\n",
      "[70/600] Loss: 0.0221 MAE: 0.0668 Mean Error: 0.0130 STD: 0.1483\n",
      "[80/600] Loss: 0.0209 MAE: 0.0652 Mean Error: 0.0124 STD: 0.1441\n",
      "[90/600] Loss: 0.0198 MAE: 0.0637 Mean Error: 0.0121 STD: 0.1401\n",
      "[100/600] Loss: 0.0187 MAE: 0.0620 Mean Error: 0.0118 STD: 0.1362\n",
      "[110/600] Loss: 0.0177 MAE: 0.0604 Mean Error: 0.0115 STD: 0.1325\n",
      "[120/600] Loss: 0.0167 MAE: 0.0588 Mean Error: 0.0112 STD: 0.1289\n",
      "[130/600] Loss: 0.0158 MAE: 0.0573 Mean Error: 0.0109 STD: 0.1254\n",
      "[140/600] Loss: 0.0150 MAE: 0.0558 Mean Error: 0.0107 STD: 0.1221\n",
      "[150/600] Loss: 0.0142 MAE: 0.0543 Mean Error: 0.0104 STD: 0.1189\n",
      "[160/600] Loss: 0.0135 MAE: 0.0529 Mean Error: 0.0102 STD: 0.1158\n",
      "[170/600] Loss: 0.0128 MAE: 0.0516 Mean Error: 0.0099 STD: 0.1128\n",
      "[180/600] Loss: 0.0122 MAE: 0.0503 Mean Error: 0.0097 STD: 0.1099\n",
      "[190/600] Loss: 0.0116 MAE: 0.0490 Mean Error: 0.0094 STD: 0.1071\n",
      "[200/600] Loss: 0.0110 MAE: 0.0478 Mean Error: 0.0092 STD: 0.1044\n",
      "[210/600] Loss: 0.0105 MAE: 0.0466 Mean Error: 0.0090 STD: 0.1019\n",
      "[220/600] Loss: 0.0100 MAE: 0.0455 Mean Error: 0.0088 STD: 0.0994\n",
      "[230/600] Loss: 0.0095 MAE: 0.0444 Mean Error: 0.0086 STD: 0.0970\n",
      "[240/600] Loss: 0.0090 MAE: 0.0434 Mean Error: 0.0084 STD: 0.0947\n",
      "[250/600] Loss: 0.0086 MAE: 0.0424 Mean Error: 0.0083 STD: 0.0926\n",
      "[260/600] Loss: 0.0082 MAE: 0.0414 Mean Error: 0.0081 STD: 0.0904\n",
      "[270/600] Loss: 0.0079 MAE: 0.0405 Mean Error: 0.0079 STD: 0.0884\n",
      "[280/600] Loss: 0.0075 MAE: 0.0396 Mean Error: 0.0078 STD: 0.0865\n",
      "[290/600] Loss: 0.0072 MAE: 0.0387 Mean Error: 0.0076 STD: 0.0846\n",
      "[300/600] Loss: 0.0069 MAE: 0.0379 Mean Error: 0.0075 STD: 0.0828\n",
      "[310/600] Loss: 0.0066 MAE: 0.0371 Mean Error: 0.0073 STD: 0.0810\n",
      "[320/600] Loss: 0.0064 MAE: 0.0363 Mean Error: 0.0072 STD: 0.0794\n",
      "[330/600] Loss: 0.0061 MAE: 0.0356 Mean Error: 0.0071 STD: 0.0778\n",
      "[340/600] Loss: 0.0059 MAE: 0.0349 Mean Error: 0.0069 STD: 0.0762\n",
      "[350/600] Loss: 0.0056 MAE: 0.0342 Mean Error: 0.0068 STD: 0.0747\n",
      "[360/600] Loss: 0.0054 MAE: 0.0336 Mean Error: 0.0067 STD: 0.0733\n",
      "[370/600] Loss: 0.0052 MAE: 0.0329 Mean Error: 0.0066 STD: 0.0719\n",
      "[380/600] Loss: 0.0050 MAE: 0.0323 Mean Error: 0.0065 STD: 0.0706\n",
      "[390/600] Loss: 0.0048 MAE: 0.0317 Mean Error: 0.0064 STD: 0.0693\n",
      "[400/600] Loss: 0.0047 MAE: 0.0311 Mean Error: 0.0063 STD: 0.0680\n",
      "[410/600] Loss: 0.0045 MAE: 0.0306 Mean Error: 0.0062 STD: 0.0668\n",
      "[420/600] Loss: 0.0043 MAE: 0.0301 Mean Error: 0.0061 STD: 0.0656\n",
      "[430/600] Loss: 0.0042 MAE: 0.0295 Mean Error: 0.0060 STD: 0.0645\n",
      "[440/600] Loss: 0.0041 MAE: 0.0290 Mean Error: 0.0059 STD: 0.0634\n",
      "[450/600] Loss: 0.0039 MAE: 0.0286 Mean Error: 0.0058 STD: 0.0623\n",
      "[460/600] Loss: 0.0038 MAE: 0.0281 Mean Error: 0.0057 STD: 0.0613\n",
      "[470/600] Loss: 0.0037 MAE: 0.0276 Mean Error: 0.0057 STD: 0.0603\n",
      "[480/600] Loss: 0.0036 MAE: 0.0272 Mean Error: 0.0056 STD: 0.0594\n",
      "[490/600] Loss: 0.0034 MAE: 0.0268 Mean Error: 0.0055 STD: 0.0584\n",
      "[500/600] Loss: 0.0033 MAE: 0.0264 Mean Error: 0.0054 STD: 0.0575\n",
      "[510/600] Loss: 0.0032 MAE: 0.0260 Mean Error: 0.0054 STD: 0.0566\n",
      "[520/600] Loss: 0.0031 MAE: 0.0256 Mean Error: 0.0053 STD: 0.0558\n",
      "[530/600] Loss: 0.0030 MAE: 0.0252 Mean Error: 0.0052 STD: 0.0550\n",
      "[540/600] Loss: 0.0030 MAE: 0.0248 Mean Error: 0.0051 STD: 0.0541\n",
      "[550/600] Loss: 0.0029 MAE: 0.0245 Mean Error: 0.0051 STD: 0.0534\n",
      "[560/600] Loss: 0.0028 MAE: 0.0241 Mean Error: 0.0050 STD: 0.0526\n",
      "[570/600] Loss: 0.0027 MAE: 0.0238 Mean Error: 0.0050 STD: 0.0519\n",
      "[580/600] Loss: 0.0026 MAE: 0.0235 Mean Error: 0.0049 STD: 0.0511\n",
      "[590/600] Loss: 0.0026 MAE: 0.0232 Mean Error: 0.0048 STD: 0.0504\n",
      "[599/600] Loss: 0.0025 MAE: 0.0229 Mean Error: 0.0048 STD: 0.0498\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "def train(dataloader, model, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        ERROR_Train = [] \n",
    "        model.train() \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            model.zero_grad()\n",
    "            real_cpu, label_cpu = data['data'], data['label']\n",
    "#             if torch.cuda.is_available():\n",
    "#                 real_cpu = real_cpu.cuda() \n",
    "#                 label_cpu = label_cpu.cuda()\n",
    "            real = real_cpu\n",
    "            label = label_cpu\n",
    "            inputv = Variable(real)\n",
    "            labelv = Variable(label)\n",
    "            output = model(inputv)\n",
    "            err = criterion(output, labelv) \n",
    "            err.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            losses.append(err.data.item())\n",
    "            error = label - output.data\n",
    "#             print(error.shape)\n",
    "            ERROR_Train.extend(error)\n",
    "#         print(ERROR_Train)\n",
    "        MAE = torch.mean(torch.abs(torch.stack(ERROR_Train)))\n",
    "        ME = torch.mean(torch.stack(ERROR_Train))\n",
    "        STD = torch.std(torch.stack(ERROR_Train)) \n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print('[%d/%d] Loss: %.4f MAE: %.4f Mean Error: %.4f STD: %.4f' % (epoch, num_epochs, np.average(losses), MAE, ME, STD))\n",
    "    return output, model\n",
    "\n",
    "# Start training        \n",
    "output, model = train(train_loader, model, num_epochs)\n",
    "# print(output.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f4d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256)\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "# Step 4a\n",
    "# Export the image after training\n",
    "# Before executing this block, create a folder called \"output\"\n",
    "if not os.path.exists('./output'):\n",
    "    os.mkdir('./output')\n",
    "output_np = output.detach().numpy()\n",
    "print(output_np.shape)\n",
    "torch.save(model, './models/net_trained.pkl')\n",
    "output_dataset = np.zeros([10, 256])\n",
    "for i in range(10):\n",
    "    output_img = output_np[i].reshape(16, 16)*255\n",
    "    img = Image.fromarray(np.uint8(output_img))\n",
    "    img = img.convert(\"1\")\n",
    "    output_path = './output/' + str(i) + '.png'\n",
    "    img.save(output_path)\n",
    "    data = img.getdata()\n",
    "    array = np.array(data)/255\n",
    "    output_dataset[i] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b891b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b\n",
    "# Calculate Fh\n",
    "def calculateFh(input_dataset, output_dataset):\n",
    "    x, y = input_dataset.shape\n",
    "    Fh_denominator = 0    # Fh分母\n",
    "    Fh_numerator = 0      # Fh分子\n",
    "    Fh_array = np.zeros([x])\n",
    "    for j in range(x):\n",
    "        for i in range(y):\n",
    "            if input_dataset[j][i] == 0:\n",
    "                Fh_denominator = Fh_denominator + 1\n",
    "                if output_dataset[j][i] == 0:\n",
    "                    Fh_numerator = Fh_numerator + 1\n",
    "        Fh = Fh_numerator / Fh_denominator\n",
    "        Fh_array[j] = Fh\n",
    "    return Fh_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3acfb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Ffa\n",
    "def calculateFfa(input_dataset, output_dataset):\n",
    "    x, y = input_dataset.shape\n",
    "    Ffa_denominator = 0    # Ffa分母\n",
    "    Ffa_numerator = 0      # Ffa分子\n",
    "    Ffa_array = np.zeros([x])\n",
    "    for j in range(x):\n",
    "        for i in range(y):\n",
    "            if input_dataset[j][i] == 1:\n",
    "                Ffa_denominator = Ffa_denominator + 1\n",
    "            if output_dataset[j][i] == 0 and input_dataset[j][i] == 1:\n",
    "                Ffa_numerator = Ffa_numerator + 1\n",
    "        Ffa = Ffa_numerator / Ffa_denominator\n",
    "        Ffa_array[j] = Ffa\n",
    "    return Ffa_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d276af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0.         0.         0.         0.0010627  0.00085106 0.00071073\n",
      " 0.0006105  0.0005322  0.00047438 0.00042753]\n"
     ]
    }
   ],
   "source": [
    "Fh_array = calculateFh(dataSet, output_dataset)\n",
    "Ffa_array = calculateFfa(dataSet, output_dataset)\n",
    "print(Fh_array)\n",
    "print(Ffa_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "097f693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4c: Graph Fh as a function of Ffa for each exemplar in the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01286dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with noise standard deviation 0.001\n",
      "[0/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[10/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[20/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[30/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[40/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[50/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[60/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[70/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[80/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[90/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[100/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[110/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[120/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[130/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[140/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[150/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[160/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[170/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[180/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[190/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[200/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[210/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[220/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[230/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[240/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[250/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[260/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[270/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[280/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[290/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[300/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[310/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[320/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[330/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[340/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[350/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[360/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[370/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[380/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[390/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[400/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[410/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[420/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[430/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[440/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[450/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[460/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[470/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[480/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[490/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[500/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[510/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[520/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[530/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[540/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[550/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[560/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[570/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[580/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[590/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "[599/600] Loss: 0.2518 MAE: 0.4870 Mean Error: 0.4016 STD: 0.3009\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.002\n",
      "[0/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[10/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[20/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[30/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[40/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[50/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[60/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[70/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[80/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[90/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[100/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[110/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[120/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[130/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[140/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[150/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[160/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[170/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[180/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[190/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[200/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[210/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[220/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[230/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[240/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[250/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[260/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[270/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[280/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[290/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[300/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[310/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[320/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[330/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[340/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[350/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[360/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[370/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[380/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[390/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[400/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[410/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[420/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[430/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[440/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[450/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[460/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[470/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[480/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[490/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[500/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[510/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[520/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[530/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[540/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[550/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[560/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[570/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[580/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[590/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "[599/600] Loss: 0.2800 MAE: 0.5143 Mean Error: 0.4293 STD: 0.3095\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.003\n",
      "[0/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[10/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[20/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[30/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[50/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[60/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[70/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[80/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[90/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[100/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[110/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[120/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[130/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[140/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[150/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[160/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[170/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[180/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[190/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[200/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[210/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[220/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[230/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[240/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[250/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[260/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[270/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[280/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[290/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[300/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[310/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[320/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[330/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[340/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[350/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[360/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[370/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[380/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[390/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[400/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[410/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[420/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[430/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[440/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[450/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[460/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[470/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[480/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[490/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[500/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[510/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[520/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[530/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[540/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[550/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[560/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[570/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[580/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[590/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "[599/600] Loss: 0.2557 MAE: 0.4881 Mean Error: 0.3974 STD: 0.3127\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.005\n",
      "[0/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[10/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[20/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[30/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[40/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[50/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[60/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[70/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[80/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[90/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[100/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[110/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[120/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[130/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[140/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[150/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[160/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[170/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[180/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[190/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[200/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[210/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[220/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[230/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[240/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[250/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[260/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[270/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[280/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[290/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[300/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[310/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[320/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[330/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[340/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[350/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[360/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[370/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[380/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[390/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[400/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[410/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[420/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[430/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[440/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[450/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[460/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[470/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[480/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[490/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[500/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[510/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[520/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[530/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[540/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[550/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[560/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[570/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[580/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[590/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "[599/600] Loss: 0.2637 MAE: 0.4976 Mean Error: 0.4077 STD: 0.3123\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.01\n",
      "[0/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[10/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[20/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[30/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[40/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[50/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[70/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[80/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[90/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[100/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[110/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[120/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[130/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[140/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[150/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[160/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[170/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[180/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[190/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[200/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[210/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[220/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[230/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[240/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[250/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[260/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[270/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[280/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[290/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[300/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[310/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[320/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[330/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[340/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[350/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[360/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[370/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[380/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[390/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[400/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[410/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[420/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[430/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[440/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[450/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[460/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[470/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[480/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[490/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[500/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[510/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[520/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[530/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[540/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[550/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[560/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[570/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[580/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[590/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "[599/600] Loss: 0.2611 MAE: 0.4940 Mean Error: 0.4090 STD: 0.3063\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.02\n",
      "[0/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[10/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[20/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[30/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[40/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[50/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[60/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[70/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[80/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[90/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[100/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[110/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[120/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[130/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[140/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[150/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[160/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[170/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[180/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[190/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[200/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[210/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[220/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[230/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[240/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[250/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[260/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[270/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[280/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[290/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[300/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[310/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[320/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[330/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[340/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[350/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[360/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[370/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[380/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[390/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[400/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[410/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[420/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[430/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[440/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[450/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[460/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[470/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[480/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[490/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[500/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[510/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[520/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[530/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[540/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[550/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[560/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[570/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[580/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[590/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "[599/600] Loss: 0.2666 MAE: 0.4993 Mean Error: 0.4171 STD: 0.3045\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.03\n",
      "[0/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[10/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[20/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[30/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[40/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[50/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[60/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[70/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[80/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[90/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[100/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[110/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[130/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[140/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[150/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[160/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[170/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[180/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[190/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[200/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[210/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[220/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[230/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[240/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[250/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[260/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[270/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[280/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[290/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[300/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[310/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[320/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[330/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[340/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[350/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[360/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[370/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[380/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[390/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[400/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[410/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[420/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[430/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[440/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[450/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[460/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[470/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[480/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[490/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[500/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[510/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[520/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[530/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[540/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[550/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[560/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[570/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[580/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[590/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "[599/600] Loss: 0.2596 MAE: 0.4956 Mean Error: 0.4069 STD: 0.3068\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.05\n",
      "[0/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[10/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[20/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[30/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[40/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[50/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[60/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[70/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[80/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[90/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[100/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[110/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[120/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[130/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[140/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[150/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[160/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[170/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[180/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[190/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[200/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[210/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[220/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[230/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[240/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[250/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[260/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[270/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[280/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[290/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[300/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[310/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[320/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[330/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[340/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[350/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[360/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[370/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[380/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[390/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[400/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[410/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[420/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[430/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[440/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[450/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[460/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[470/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[480/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[490/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[500/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[510/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[520/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[530/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[540/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[550/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[560/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[570/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[580/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[590/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "[599/600] Loss: 0.2698 MAE: 0.5011 Mean Error: 0.4187 STD: 0.3075\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.1\n",
      "[0/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[10/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[20/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[30/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[40/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[50/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[60/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[70/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[80/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[90/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[100/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[110/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[130/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[140/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[150/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[160/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[170/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[180/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[190/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[200/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[210/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[220/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[230/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[240/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[250/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[260/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[270/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[280/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[290/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[300/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[310/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[320/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[330/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[340/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[350/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[360/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[370/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[380/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[390/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[400/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[410/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[420/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[430/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[440/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[450/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[460/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[470/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[480/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[490/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[500/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[510/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[520/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[530/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[540/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[550/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[560/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[570/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[580/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[590/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "[599/600] Loss: 0.2625 MAE: 0.4957 Mean Error: 0.4060 STD: 0.3126\n",
      "------------------------------------\n",
      "------------Fh_noise_array------------\n",
      "[[0.44       0.51351351 0.5        0.46987952 0.45714286 0.46511628\n",
      "  0.45454545 0.46153846 0.4744898  0.47963801]\n",
      " [0.48       0.54054054 0.55       0.54216867 0.54285714 0.53488372\n",
      "  0.51948052 0.52662722 0.51530612 0.5158371 ]\n",
      " [0.4        0.40540541 0.45       0.45783133 0.46666667 0.46511628\n",
      "  0.45454545 0.46153846 0.46938776 0.47058824]\n",
      " [0.36       0.43243243 0.41666667 0.43373494 0.42857143 0.42635659\n",
      "  0.45454545 0.46153846 0.46428571 0.47058824]\n",
      " [0.52       0.48648649 0.48333333 0.5060241  0.5047619  0.51162791\n",
      "  0.50649351 0.5147929  0.51020408 0.50678733]\n",
      " [0.52       0.51351351 0.51666667 0.51807229 0.4952381  0.51162791\n",
      "  0.51298701 0.52662722 0.53061224 0.53846154]\n",
      " [0.56       0.54054054 0.55       0.54216867 0.53333333 0.53488372\n",
      "  0.54545455 0.53846154 0.53571429 0.53393665]\n",
      " [0.56       0.56756757 0.56666667 0.56626506 0.55238095 0.54263566\n",
      "  0.54545455 0.5443787  0.54081633 0.52941176]\n",
      " [0.48       0.45945946 0.5        0.4939759  0.4952381  0.49612403\n",
      "  0.5        0.50887574 0.51020408 0.49773756]]\n",
      "------------Ffa_noise_array------------\n",
      "[[0.5021645  0.49263158 0.49293785 0.49521785 0.49531915 0.49466951\n",
      "  0.496337   0.49547632 0.4943074  0.49380077]\n",
      " [0.51948052 0.51578947 0.51694915 0.51647184 0.51659574 0.51741294\n",
      "  0.51892552 0.51836083 0.51897533 0.51902522]\n",
      " [0.49350649 0.49263158 0.49011299 0.48884166 0.48595745 0.48614072\n",
      "  0.48840049 0.48749335 0.48766603 0.48781531]\n",
      " [0.51082251 0.50315789 0.50706215 0.50584485 0.50638298 0.50604122\n",
      "  0.5030525  0.50186269 0.50142315 0.50106883]\n",
      " [0.5021645  0.50105263 0.50141243 0.49946865 0.49957447 0.49964463\n",
      "  0.498779   0.49707291 0.49762808 0.49722103]\n",
      " [0.5021645  0.50315789 0.50564972 0.50690755 0.50808511 0.50675195\n",
      "  0.50671551 0.50612028 0.50616698 0.50491663]\n",
      " [0.48484848 0.49052632 0.48870056 0.48990436 0.4893617  0.48898365\n",
      "  0.48840049 0.49015434 0.49003795 0.4903805 ]\n",
      " [0.5021645  0.50315789 0.50423729 0.50371945 0.50468085 0.50533049\n",
      "  0.504884   0.50558808 0.5056926  0.5070543 ]\n",
      " [0.5021645  0.49473684 0.49152542 0.49309245 0.49361702 0.49395878\n",
      "  0.49328449 0.49228313 0.49193548 0.4929457 ]]\n"
     ]
    }
   ],
   "source": [
    "# Step 5\n",
    "def gaussian_noise(img, mean, sigma):\n",
    "    # Generate gauss noise\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    # Add the noise to image\n",
    "    gaussian_out = img + noise\n",
    "    # Make the value between 0 and 1\n",
    "    gaussian_out = np.clip(gaussian_out, 0, 1)\n",
    "    return gaussian_out\n",
    "\n",
    "gaussian_dataset = np.zeros([9, 10, 256])\n",
    "std = [0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "if not os.path.exists('./input_noise/'):\n",
    "    os.mkdir('./input_noise/')\n",
    "for j in range(9):\n",
    "    if not os.path.exists('./input_noise/' + str(std[j])):\n",
    "        os.mkdir('./input_noise/' + str(std[j]))\n",
    "    for i in range(10):\n",
    "        inputImage = dataSet[i]\n",
    "        gaussian_data = gaussian_noise(inputImage, 0, std[j])\n",
    "        img = gaussian_data.reshape(16, 16)*255\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        img.convert(\"1\")\n",
    "        inputImageDir = './input_noise/' + str(std[j]) + '/' + str(i) + '.png'\n",
    "        img.save(inputImageDir)\n",
    "        gaussian_dataset[j][i] = gaussian_data\n",
    "gaussian_dataset = np.array(gaussian_dataset)\n",
    "\n",
    "\n",
    "Fh_noise_array = np.zeros([9, 10])\n",
    "Ffa_noise_array = np.zeros([9, 10])\n",
    "\n",
    "# Train 9 datasets with noise\n",
    "if not os.path.exists('./output_noise/'):\n",
    "    os.mkdir('./output_noise/')\n",
    "for j in range(9):\n",
    "    train_noise_dataset = DigitDataset(dataset = gaussian_dataset[j], label_list = dataSet)\n",
    "    train_noise_loader = DataLoader(dataset=train_noise_dataset, batch_size=batch_size, shuffle=False)\n",
    "    print('Training dataset with noise standard deviation ' + str(std[j]))\n",
    "#     model_noise = torch.load('./models/net_untrained.pkl') #  Load the model that trained before\n",
    "    model_noise = Perceptron(input_size=input_size, num_classes=num_classes).to(device)\n",
    "    output_noise, model_noise = train(train_noise_loader, model_noise, num_epochs)   # Train\n",
    "    torch.save(model_noise, './models/net_trained_' + str(std[j]) + '.pkl')\n",
    "    model_noise = torch.load('./models/net_trained_' + str(std[j]) + '.pkl')\n",
    "#     output_noise = model_noise(torch.from_numpy(gaussian_dataset[j]).float()) # Use the model trained before to test\n",
    "    print('------------------------------------')\n",
    "    output_noise = model_noise(torch.from_numpy(dataSet).float())\n",
    "    output_noise_np = output_noise.detach().numpy()     # Get the output\n",
    "#     print(output_noise_np)\n",
    "    output_noise_dataset = np.zeros([10, 256])\n",
    "#     Make the output only has 0 or 1\n",
    "    \n",
    "    if not os.path.exists('./output_noise/' + str(std[j])):\n",
    "        os.mkdir('./output_noise/' + str(std[j]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(10):\n",
    "        output_noise_img = output_noise_np[i].reshape(16, 16)*255\n",
    "        img = Image.fromarray(np.uint8(output_noise_img))\n",
    "        img = img.convert(\"1\")\n",
    "        output_path = './output_noise/' + str(std[j]) + '/' + str(i) + '.png'\n",
    "        img.save(output_path)\n",
    "        data = img.getdata()\n",
    "        array = np.array(data)/255\n",
    "        output_noise_dataset[i] = array\n",
    "#     Calculate Fh and Ffa\n",
    "    Fh = calculateFh(dataSet, output_noise_dataset)\n",
    "    Ffa = calculateFfa(dataSet, output_noise_dataset)\n",
    "    Fh_noise_array[j] = Fh\n",
    "    Ffa_noise_array[j] = Ffa\n",
    "print('------------Fh_noise_array------------')\n",
    "print(Fh_noise_array)\n",
    "print('------------Ffa_noise_array------------')\n",
    "print(Ffa_noise_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4010e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 6: Display Data from your Tests in Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b32ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
