{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcd5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11838ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Get the image and change every image to an 256-dimention vector\n",
    "dataSet = np.zeros([10, 256])\n",
    "for i in range(0, 10):\n",
    "    inputImageDir = './input/' + str(i) + '.png'\n",
    "    inputImage = Image.open(inputImageDir)\n",
    "    inputImage = inputImage.convert(\"1\")\n",
    "    inputImage.save(inputImageDir)\n",
    "    data = inputImage.getdata()\n",
    "    array = np.array(data)/255\n",
    "    dataSet[i] = array\n",
    "dataSet = np.array(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1478743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# define a neural network\n",
    "class Perceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_hidden, num_classes):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.inputlayer = nn.Linear(input_size, num_hidden)\n",
    "        self.hiddenlayer = nn.Linear(num_hidden, num_hidden)\n",
    "        self.outputlayer = nn.Linear(num_hidden, num_classes)\n",
    "        self.activate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.inputlayer(x)\n",
    "        res = self.activate(res)\n",
    "        res = self.hiddenlayer(x)\n",
    "        res = self.activate(res)\n",
    "        res = self.outputlayer(x)\n",
    "        res = self.activate(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "634a9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, dataset, label_list):\n",
    "        self.dataset = dataset\n",
    "        self.label_list = label_list\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        label = self.label_list[idx]\n",
    "        return {\n",
    "            'data': torch.from_numpy(data).float(),\n",
    "            'label': torch.from_numpy(label).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc25a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of training\n",
    "input_size = 256\n",
    "num_classes = 256\n",
    "num_hidden = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 600\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = DigitDataset(dataset = dataSet, label_list = dataSet)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cpu')\n",
    "model = Perceptron(input_size=input_size, num_hidden=num_hidden, num_classes=num_classes).to(device)\n",
    "\n",
    "if not os.path.exists('./models'):\n",
    "    os.mkdir('./models')\n",
    "torch.save(model, './models/net_untrained.pkl')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "452b7bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/600] Loss: 0.2617 MAE: 0.4941 Mean Error: 0.4047 STD: 0.3130\n",
      "[10/600] Loss: 0.0435 MAE: 0.1581 Mean Error: 0.0970 STD: 0.1847\n",
      "[20/600] Loss: 0.0303 MAE: 0.0915 Mean Error: 0.0328 STD: 0.1711\n",
      "[30/600] Loss: 0.0279 MAE: 0.0775 Mean Error: 0.0204 STD: 0.1657\n",
      "[40/600] Loss: 0.0261 MAE: 0.0726 Mean Error: 0.0165 STD: 0.1608\n",
      "[50/600] Loss: 0.0246 MAE: 0.0702 Mean Error: 0.0153 STD: 0.1563\n",
      "[60/600] Loss: 0.0233 MAE: 0.0682 Mean Error: 0.0139 STD: 0.1519\n",
      "[70/600] Loss: 0.0220 MAE: 0.0667 Mean Error: 0.0132 STD: 0.1477\n",
      "[80/600] Loss: 0.0208 MAE: 0.0654 Mean Error: 0.0128 STD: 0.1436\n",
      "[90/600] Loss: 0.0196 MAE: 0.0638 Mean Error: 0.0123 STD: 0.1396\n",
      "[100/600] Loss: 0.0186 MAE: 0.0622 Mean Error: 0.0121 STD: 0.1358\n",
      "[110/600] Loss: 0.0176 MAE: 0.0605 Mean Error: 0.0118 STD: 0.1320\n",
      "[120/600] Loss: 0.0166 MAE: 0.0590 Mean Error: 0.0114 STD: 0.1284\n",
      "[130/600] Loss: 0.0157 MAE: 0.0575 Mean Error: 0.0112 STD: 0.1249\n",
      "[140/600] Loss: 0.0149 MAE: 0.0560 Mean Error: 0.0109 STD: 0.1215\n",
      "[150/600] Loss: 0.0141 MAE: 0.0545 Mean Error: 0.0106 STD: 0.1182\n",
      "[160/600] Loss: 0.0133 MAE: 0.0531 Mean Error: 0.0104 STD: 0.1150\n",
      "[170/600] Loss: 0.0126 MAE: 0.0517 Mean Error: 0.0101 STD: 0.1119\n",
      "[180/600] Loss: 0.0120 MAE: 0.0504 Mean Error: 0.0099 STD: 0.1089\n",
      "[190/600] Loss: 0.0113 MAE: 0.0491 Mean Error: 0.0097 STD: 0.1061\n",
      "[200/600] Loss: 0.0108 MAE: 0.0479 Mean Error: 0.0095 STD: 0.1034\n",
      "[210/600] Loss: 0.0102 MAE: 0.0467 Mean Error: 0.0093 STD: 0.1008\n",
      "[220/600] Loss: 0.0097 MAE: 0.0455 Mean Error: 0.0091 STD: 0.0982\n",
      "[230/600] Loss: 0.0093 MAE: 0.0444 Mean Error: 0.0089 STD: 0.0958\n",
      "[240/600] Loss: 0.0088 MAE: 0.0433 Mean Error: 0.0087 STD: 0.0935\n",
      "[250/600] Loss: 0.0084 MAE: 0.0423 Mean Error: 0.0085 STD: 0.0913\n",
      "[260/600] Loss: 0.0080 MAE: 0.0413 Mean Error: 0.0084 STD: 0.0892\n",
      "[270/600] Loss: 0.0077 MAE: 0.0404 Mean Error: 0.0082 STD: 0.0872\n",
      "[280/600] Loss: 0.0073 MAE: 0.0395 Mean Error: 0.0080 STD: 0.0852\n",
      "[290/600] Loss: 0.0070 MAE: 0.0386 Mean Error: 0.0079 STD: 0.0833\n",
      "[300/600] Loss: 0.0067 MAE: 0.0378 Mean Error: 0.0077 STD: 0.0815\n",
      "[310/600] Loss: 0.0064 MAE: 0.0370 Mean Error: 0.0076 STD: 0.0798\n",
      "[320/600] Loss: 0.0062 MAE: 0.0362 Mean Error: 0.0075 STD: 0.0781\n",
      "[330/600] Loss: 0.0059 MAE: 0.0355 Mean Error: 0.0073 STD: 0.0765\n",
      "[340/600] Loss: 0.0057 MAE: 0.0348 Mean Error: 0.0072 STD: 0.0750\n",
      "[350/600] Loss: 0.0055 MAE: 0.0341 Mean Error: 0.0071 STD: 0.0735\n",
      "[360/600] Loss: 0.0052 MAE: 0.0334 Mean Error: 0.0070 STD: 0.0721\n",
      "[370/600] Loss: 0.0050 MAE: 0.0328 Mean Error: 0.0068 STD: 0.0707\n",
      "[380/600] Loss: 0.0049 MAE: 0.0322 Mean Error: 0.0067 STD: 0.0694\n",
      "[390/600] Loss: 0.0047 MAE: 0.0316 Mean Error: 0.0066 STD: 0.0681\n",
      "[400/600] Loss: 0.0045 MAE: 0.0310 Mean Error: 0.0065 STD: 0.0669\n",
      "[410/600] Loss: 0.0044 MAE: 0.0305 Mean Error: 0.0064 STD: 0.0657\n",
      "[420/600] Loss: 0.0042 MAE: 0.0299 Mean Error: 0.0063 STD: 0.0645\n",
      "[430/600] Loss: 0.0041 MAE: 0.0294 Mean Error: 0.0062 STD: 0.0634\n",
      "[440/600] Loss: 0.0039 MAE: 0.0289 Mean Error: 0.0061 STD: 0.0623\n",
      "[450/600] Loss: 0.0038 MAE: 0.0284 Mean Error: 0.0060 STD: 0.0613\n",
      "[460/600] Loss: 0.0037 MAE: 0.0280 Mean Error: 0.0060 STD: 0.0603\n",
      "[470/600] Loss: 0.0035 MAE: 0.0275 Mean Error: 0.0059 STD: 0.0593\n",
      "[480/600] Loss: 0.0034 MAE: 0.0271 Mean Error: 0.0058 STD: 0.0583\n",
      "[490/600] Loss: 0.0033 MAE: 0.0267 Mean Error: 0.0057 STD: 0.0574\n",
      "[500/600] Loss: 0.0032 MAE: 0.0263 Mean Error: 0.0056 STD: 0.0565\n",
      "[510/600] Loss: 0.0031 MAE: 0.0259 Mean Error: 0.0056 STD: 0.0557\n",
      "[520/600] Loss: 0.0030 MAE: 0.0255 Mean Error: 0.0055 STD: 0.0548\n",
      "[530/600] Loss: 0.0029 MAE: 0.0251 Mean Error: 0.0054 STD: 0.0540\n",
      "[540/600] Loss: 0.0029 MAE: 0.0247 Mean Error: 0.0054 STD: 0.0532\n",
      "[550/600] Loss: 0.0028 MAE: 0.0244 Mean Error: 0.0053 STD: 0.0524\n",
      "[560/600] Loss: 0.0027 MAE: 0.0240 Mean Error: 0.0052 STD: 0.0517\n",
      "[570/600] Loss: 0.0026 MAE: 0.0237 Mean Error: 0.0052 STD: 0.0510\n",
      "[580/600] Loss: 0.0026 MAE: 0.0234 Mean Error: 0.0051 STD: 0.0503\n",
      "[590/600] Loss: 0.0025 MAE: 0.0231 Mean Error: 0.0050 STD: 0.0496\n",
      "[599/600] Loss: 0.0024 MAE: 0.0228 Mean Error: 0.0050 STD: 0.0490\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "def train(dataloader, model, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        ERROR_Train = [] \n",
    "        model.train() \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            model.zero_grad()\n",
    "            real_cpu, label_cpu = data['data'], data['label']\n",
    "#             if torch.cuda.is_available():\n",
    "#                 real_cpu = real_cpu.cuda() \n",
    "#                 label_cpu = label_cpu.cuda()\n",
    "            real = real_cpu\n",
    "            label = label_cpu\n",
    "            inputv = Variable(real)\n",
    "            labelv = Variable(label)\n",
    "            output = model(inputv)\n",
    "            err = criterion(output, labelv) \n",
    "            err.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            losses.append(err.data.item())\n",
    "            error = label - output.data\n",
    "#             print(error.shape)\n",
    "            ERROR_Train.extend(error)\n",
    "#         print(ERROR_Train)\n",
    "        MAE = torch.mean(torch.abs(torch.stack(ERROR_Train)))\n",
    "        ME = torch.mean(torch.stack(ERROR_Train))\n",
    "        STD = torch.std(torch.stack(ERROR_Train)) \n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print('[%d/%d] Loss: %.4f MAE: %.4f Mean Error: %.4f STD: %.4f' % (epoch, num_epochs, np.average(losses), MAE, ME, STD))\n",
    "    return output, model\n",
    "\n",
    "# Start training        \n",
    "output, model = train(train_loader, model, num_epochs)\n",
    "# print(output.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f4d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256)\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "# Step 4a\n",
    "# Export the image after training\n",
    "# Before executing this block, create a folder called \"output\"\n",
    "if not os.path.exists('./output'):\n",
    "    os.mkdir('./output')\n",
    "output_np = output.detach().numpy()\n",
    "print(output_np.shape)\n",
    "torch.save(model, './models/net_trained.pkl')\n",
    "output_dataset = np.zeros([10, 256])\n",
    "for i in range(10):\n",
    "    output_img = output_np[i].reshape(16, 16)*255\n",
    "    img = Image.fromarray(np.uint8(output_img))\n",
    "    img = img.convert(\"1\")\n",
    "    output_path = './output/' + str(i) + '.png'\n",
    "    img.save(output_path)\n",
    "    data = img.getdata()\n",
    "    array = np.array(data)/255\n",
    "    output_dataset[i] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b891b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4b\n",
    "# Calculate Fh\n",
    "def calculateFh(input_dataset, output_dataset):\n",
    "    x, y = input_dataset.shape\n",
    "    Fh_denominator = 0    # Fh分母\n",
    "    Fh_numerator = 0      # Fh分子\n",
    "    Fh_array = np.zeros([x])\n",
    "    for j in range(x):\n",
    "        for i in range(y):\n",
    "            if input_dataset[j][i] == 0:\n",
    "                Fh_denominator = Fh_denominator + 1\n",
    "                if output_dataset[j][i] == 0:\n",
    "                    Fh_numerator = Fh_numerator + 1\n",
    "        Fh = Fh_numerator / Fh_denominator\n",
    "        Fh_array[j] = Fh\n",
    "    return Fh_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3acfb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Ffa\n",
    "def calculateFfa(input_dataset, output_dataset):\n",
    "    x, y = input_dataset.shape\n",
    "    Ffa_denominator = 0    # Ffa分母\n",
    "    Ffa_numerator = 0      # Ffa分子\n",
    "    Ffa_array = np.zeros([x])\n",
    "    for j in range(x):\n",
    "        for i in range(y):\n",
    "            if input_dataset[j][i] == 1:\n",
    "                Ffa_denominator = Ffa_denominator + 1\n",
    "            if output_dataset[j][i] == 0 and input_dataset[j][i] == 1:\n",
    "                Ffa_numerator = Ffa_numerator + 1\n",
    "        Ffa = Ffa_numerator / Ffa_denominator\n",
    "        Ffa_array[j] = Ffa\n",
    "    return Ffa_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d276af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0.         0.         0.         0.0010627  0.00085106 0.00071073\n",
      " 0.0006105  0.0005322  0.00047438 0.00042753]\n"
     ]
    }
   ],
   "source": [
    "Fh_array = calculateFh(dataSet, output_dataset)\n",
    "Ffa_array = calculateFfa(dataSet, output_dataset)\n",
    "print(Fh_array)\n",
    "print(Ffa_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "097f693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4c: Graph Fh as a function of Ffa for each exemplar in the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01286dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with noise standard deviation 0.001\n",
      "[0/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[10/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[20/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[30/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[40/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[50/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[60/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[70/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[80/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[90/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[100/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[110/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[120/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[130/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[140/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[150/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[160/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[170/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[180/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[190/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[200/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[210/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[220/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[230/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[240/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[250/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[260/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[270/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[280/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[290/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[300/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[310/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[320/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[330/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[340/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[350/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[360/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[370/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[380/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[390/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[400/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[410/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[420/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[430/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[440/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[450/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[460/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[470/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[480/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[490/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[500/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[510/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[520/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[530/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[540/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[550/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[560/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[570/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[580/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[590/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "[599/600] Loss: 0.2678 MAE: 0.5015 Mean Error: 0.4119 STD: 0.3133\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.002\n",
      "[0/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[10/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[20/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[30/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[40/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[50/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[60/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[70/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[80/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[90/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[100/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[110/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[120/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[130/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[140/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[150/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[160/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[170/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[180/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[190/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[200/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[210/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[220/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[230/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[240/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[250/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[260/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[270/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[280/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[290/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[300/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[310/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[320/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[330/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[340/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[350/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[360/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[370/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[380/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[390/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[400/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[410/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[420/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[430/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[440/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[450/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[460/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[470/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[480/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[490/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[500/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[510/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[520/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[530/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[540/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[550/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[560/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[570/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[580/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[590/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "[599/600] Loss: 0.2620 MAE: 0.4957 Mean Error: 0.4081 STD: 0.3091\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[10/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[20/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[30/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[40/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[50/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[60/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[70/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[80/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[90/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[100/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[110/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[120/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[130/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[140/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[150/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[160/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[170/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[180/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[190/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[200/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[210/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[220/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[230/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[240/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[250/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[260/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[270/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[280/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[290/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[300/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[310/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[320/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[330/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[340/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[350/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[360/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[370/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[380/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[390/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[400/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[410/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[420/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[430/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[440/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[450/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[460/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[470/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[480/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[490/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[500/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[510/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[520/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[530/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[540/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[550/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[560/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[570/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[580/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[590/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "[599/600] Loss: 0.2640 MAE: 0.4990 Mean Error: 0.4130 STD: 0.3057\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.005\n",
      "[0/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[10/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[20/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[30/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[40/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[50/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[60/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[70/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[80/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[90/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[100/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[110/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[120/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[130/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[140/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[150/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[160/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[170/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[180/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[190/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[200/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[210/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[220/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[230/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[240/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[250/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[260/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[270/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[280/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[290/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[300/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[310/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[320/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[330/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[340/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[350/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[360/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[370/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[380/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[390/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[400/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[410/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[420/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[430/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[440/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[450/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[460/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[470/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[480/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[490/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[500/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[510/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[520/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[530/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[540/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[550/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[560/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[570/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[580/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[590/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "[599/600] Loss: 0.2571 MAE: 0.4896 Mean Error: 0.3969 STD: 0.3156\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.01\n",
      "[0/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[10/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[20/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[30/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[50/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[60/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[70/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[80/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[90/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[100/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[110/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[120/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[130/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[140/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[150/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[160/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[170/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[180/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[190/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[200/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[210/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[220/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[230/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[240/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[250/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[260/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[270/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[280/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[290/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[300/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[310/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[320/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[330/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[340/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[350/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[360/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[370/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[380/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[390/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[400/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[410/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[420/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[430/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[440/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[450/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[460/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[470/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[480/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[490/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[500/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[510/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[520/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[530/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[540/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[550/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[560/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[570/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[580/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[590/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "[599/600] Loss: 0.2655 MAE: 0.4984 Mean Error: 0.4145 STD: 0.3061\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.02\n",
      "[0/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[10/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[20/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[30/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[40/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[50/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[60/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[70/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[80/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[90/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[100/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[110/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[120/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[130/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[140/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[150/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[160/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[170/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[180/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[190/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[200/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[210/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[220/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[230/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[240/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[250/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[260/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[270/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[280/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[290/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[300/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[310/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[320/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[330/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[340/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[350/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[360/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[370/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[380/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[390/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[400/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[410/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[420/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[430/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[440/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[450/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[460/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[470/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[480/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[490/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[500/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[510/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[520/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[530/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[540/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[550/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[560/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[570/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[580/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[590/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "[599/600] Loss: 0.2670 MAE: 0.5007 Mean Error: 0.4140 STD: 0.3093\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.03\n",
      "[0/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[10/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[20/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[30/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[40/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[50/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[70/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[80/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[90/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[100/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[110/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[120/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[130/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[140/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[150/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[160/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[170/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[180/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[190/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[200/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[210/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[220/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[230/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[240/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[250/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[260/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[270/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[280/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[290/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[300/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[310/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[320/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[330/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[340/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[350/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[360/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[370/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[380/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[390/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[400/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[410/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[420/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[430/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[440/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[450/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[460/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[470/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[480/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[490/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[500/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[510/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[520/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[530/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[540/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[550/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[560/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[570/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[580/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[590/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "[599/600] Loss: 0.2599 MAE: 0.4933 Mean Error: 0.4083 STD: 0.3054\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.05\n",
      "[0/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[10/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[20/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[30/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[40/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[50/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[60/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[70/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[80/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[90/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[100/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[110/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[120/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[130/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[140/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[150/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[160/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[170/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[180/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[190/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[200/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[210/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[220/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[230/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[240/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[250/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[260/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[270/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[280/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[290/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[300/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[310/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[320/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[330/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[340/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[350/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[360/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[370/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[380/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[390/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[400/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[410/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[420/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[430/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[440/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[450/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[460/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[470/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[480/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[490/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[500/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[510/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[520/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[530/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[540/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[550/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[560/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[570/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[580/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[590/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "[599/600] Loss: 0.2733 MAE: 0.5060 Mean Error: 0.4216 STD: 0.3092\n",
      "------------------------------------\n",
      "Training dataset with noise standard deviation 0.1\n",
      "[0/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[10/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[20/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[30/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[40/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[50/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[60/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[80/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[90/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[100/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[110/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[120/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[130/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[140/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[150/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[160/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[170/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[180/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[190/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[200/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[210/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[220/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[230/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[240/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[250/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[260/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[270/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[280/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[290/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[300/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[310/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[320/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[330/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[340/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[350/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[360/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[370/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[380/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[390/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[400/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[410/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[420/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[430/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[440/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[450/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[460/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[470/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[480/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[490/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[500/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[510/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[520/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[530/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[540/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[550/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[560/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[570/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[580/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[590/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "[599/600] Loss: 0.2624 MAE: 0.4955 Mean Error: 0.4146 STD: 0.3009\n",
      "------------------------------------\n",
      "------------Fh_noise_array------------\n",
      "[[0.44       0.48648649 0.5        0.4939759  0.5047619  0.50387597\n",
      "  0.49350649 0.49112426 0.5        0.48868778]\n",
      " [0.48       0.48648649 0.5        0.48192771 0.4952381  0.48837209\n",
      "  0.48701299 0.49112426 0.48469388 0.47963801]\n",
      " [0.56       0.54054054 0.53333333 0.53012048 0.51428571 0.51937984\n",
      "  0.51948052 0.5147929  0.52040816 0.52036199]\n",
      " [0.4        0.37837838 0.4        0.39759036 0.41904762 0.42635659\n",
      "  0.42207792 0.42011834 0.43367347 0.42986425]\n",
      " [0.48       0.43243243 0.48333333 0.46987952 0.48571429 0.48837209\n",
      "  0.50649351 0.50295858 0.51020408 0.50678733]\n",
      " [0.48       0.45945946 0.46666667 0.45783133 0.46666667 0.46511628\n",
      "  0.46103896 0.46745562 0.46428571 0.47058824]\n",
      " [0.52       0.51351351 0.48333333 0.48192771 0.47619048 0.47286822\n",
      "  0.47402597 0.47928994 0.48979592 0.49321267]\n",
      " [0.56       0.51351351 0.55       0.51807229 0.52380952 0.51937984\n",
      "  0.51948052 0.52662722 0.52040816 0.52488688]\n",
      " [0.52       0.56756757 0.53333333 0.51807229 0.53333333 0.53488372\n",
      "  0.53246753 0.5443787  0.53571429 0.54298643]]\n",
      "------------Ffa_noise_array------------\n",
      "[[0.5021645  0.49684211 0.49858757 0.49840595 0.49702128 0.49680171\n",
      "  0.497558   0.49866951 0.49762808 0.4980761 ]\n",
      " [0.4978355  0.49684211 0.49576271 0.49734325 0.49617021 0.49680171\n",
      "  0.4969475  0.49654071 0.4971537  0.49850363]\n",
      " [0.49350649 0.49894737 0.5        0.49946865 0.50042553 0.49964463\n",
      "  0.5        0.50133049 0.5        0.50021377]\n",
      " [0.48917749 0.49052632 0.49152542 0.49309245 0.49106383 0.49040512\n",
      "  0.49084249 0.49121873 0.49051233 0.49123557]\n",
      " [0.5021645  0.50526316 0.50141243 0.50371945 0.50212766 0.50177683\n",
      "  0.5        0.5002661  0.49952562 0.49978623]\n",
      " [0.49350649 0.50315789 0.50282486 0.50478215 0.50553191 0.50604122\n",
      "  0.50610501 0.50612028 0.50616698 0.50534416]\n",
      " [0.4978355  0.49684211 0.49858757 0.49734325 0.49787234 0.4989339\n",
      "  0.4981685  0.49760511 0.4971537  0.49636597]\n",
      " [0.51082251 0.51157895 0.50847458 0.51222104 0.51148936 0.51172708\n",
      "  0.51159951 0.51091006 0.5113852  0.51090209]\n",
      " [0.50649351 0.50315789 0.50423729 0.50690755 0.50553191 0.50461976\n",
      "  0.5042735  0.50399148 0.50474383 0.50363403]]\n"
     ]
    }
   ],
   "source": [
    "# Step 5\n",
    "def gaussian_noise(img, mean, sigma):\n",
    "    # Generate gauss noise\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    # Add the noise to image\n",
    "    gaussian_out = img + noise\n",
    "    # Make the value between 0 and 1\n",
    "    gaussian_out = np.clip(gaussian_out, 0, 1)\n",
    "    return gaussian_out\n",
    "\n",
    "gaussian_dataset = np.zeros([9, 10, 256])\n",
    "std = [0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "if not os.path.exists('./input_noise/'):\n",
    "    os.mkdir('./input_noise/')\n",
    "for j in range(9):\n",
    "    if not os.path.exists('./input_noise/' + str(std[j])):\n",
    "        os.mkdir('./input_noise/' + str(std[j]))\n",
    "    for i in range(10):\n",
    "        inputImage = dataSet[i]\n",
    "        gaussian_data = gaussian_noise(inputImage, 0, std[j])\n",
    "        img = gaussian_data.reshape(16, 16)*255\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        img.convert(\"1\")\n",
    "        inputImageDir = './input_noise/' + str(std[j]) + '/' + str(i) + '.png'\n",
    "        img.save(inputImageDir)\n",
    "        gaussian_dataset[j][i] = gaussian_data\n",
    "gaussian_dataset = np.array(gaussian_dataset)\n",
    "\n",
    "\n",
    "Fh_noise_array = np.zeros([9, 10])\n",
    "Ffa_noise_array = np.zeros([9, 10])\n",
    "\n",
    "# Train 9 datasets with noise\n",
    "if not os.path.exists('./output_noise/'):\n",
    "    os.mkdir('./output_noise/')\n",
    "for j in range(9):\n",
    "    train_noise_dataset = DigitDataset(dataset = gaussian_dataset[j], label_list = dataSet)\n",
    "    train_noise_loader = DataLoader(dataset=train_noise_dataset, batch_size=batch_size, shuffle=False)\n",
    "    print('Training dataset with noise standard deviation ' + str(std[j]))\n",
    "#     model_noise = torch.load('./models/net_untrained.pkl') #  Load the model that trained before\n",
    "    model_noise = Perceptron(input_size=input_size, num_hidden=num_hidden, num_classes=num_classes).to(device)\n",
    "    output_noise, model_noise = train(train_noise_loader, model_noise, num_epochs)   # Train\n",
    "    torch.save(model_noise, './models/net_trained_' + str(std[j]) + '.pkl')\n",
    "    model_noise = torch.load('./models/net_trained_' + str(std[j]) + '.pkl')\n",
    "#     output_noise = model_noise(torch.from_numpy(gaussian_dataset[j]).float()) # Use the model trained before to test\n",
    "    print('------------------------------------')\n",
    "    output_noise = model_noise(torch.from_numpy(dataSet).float())\n",
    "    output_noise_np = output_noise.detach().numpy()     # Get the output\n",
    "#     print(output_noise_np)\n",
    "    output_noise_dataset = np.zeros([10, 256])\n",
    "#     Make the output only has 0 or 1\n",
    "    \n",
    "    if not os.path.exists('./output_noise/' + str(std[j])):\n",
    "        os.mkdir('./output_noise/' + str(std[j]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(10):\n",
    "        output_noise_img = output_noise_np[i].reshape(16, 16)*255\n",
    "        img = Image.fromarray(np.uint8(output_noise_img))\n",
    "        img = img.convert(\"1\")\n",
    "        output_path = './output_noise/' + str(std[j]) + '/' + str(i) + '.png'\n",
    "        img.save(output_path)\n",
    "        data = img.getdata()\n",
    "        array = np.array(data)/255\n",
    "        output_noise_dataset[i] = array\n",
    "#     Calculate Fh and Ffa\n",
    "    Fh = calculateFh(dataSet, output_noise_dataset)\n",
    "    Ffa = calculateFfa(dataSet, output_noise_dataset)\n",
    "    Fh_noise_array[j] = Fh\n",
    "    Ffa_noise_array[j] = Ffa\n",
    "print('------------Fh_noise_array------------')\n",
    "print(Fh_noise_array)\n",
    "print('------------Ffa_noise_array------------')\n",
    "print(Ffa_noise_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4010e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 6: Display Data from your Tests in Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b32ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
